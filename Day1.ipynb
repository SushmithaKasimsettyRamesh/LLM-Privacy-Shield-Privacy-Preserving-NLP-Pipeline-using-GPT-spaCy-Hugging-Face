{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNto19SYYHkdtb0i7l3e0PO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushmithaKasimsettyRamesh/LLM-Privacy-Shield-Privacy-Preserving-NLP-Pipeline-using-GPT-spaCy-Hugging-Face/blob/main/Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-PIBJLjV8r7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üéØ Goal: Detect PII and replace with tokens, maintaining mapping for restoration\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP & INSTALLATION\n",
        "# ============================================================================\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5gRaN825WAri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy python-dotenv\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fS2LszvWIzF",
        "outputId": "b971b332-e913-4bf9-e5fb-c273e13f8e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.6.15)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.1\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "rGkmvtcvWeg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "W-tskDonW8xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CORE ANONYMIZER CLASS\n",
        "# ============================================================================\n",
        "@dataclass\n",
        "class PIIMatch:\n",
        "    \"\"\"Represents a detected PII entity\"\"\"\n",
        "    text: str\n",
        "    label: str\n",
        "    start: int\n",
        "    end: int\n",
        "    token: str\n",
        "\n",
        "class PIIAnonymizer:\n",
        "    \"\"\"\n",
        "    Core anonymizer that detects PII and replaces with tokens\n",
        "    Maintains bidirectional mapping for restoration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Token counters for consistent mapping\n",
        "        self.token_counters = defaultdict(int)\n",
        "\n",
        "        # Bidirectional mapping\n",
        "        self.real_to_token = {}  # \"John Smith\" -> \"{{NAME_1}}\"\n",
        "        self.token_to_real = {}  # \"{{NAME_1}}\" -> \"John Smith\"\n",
        "\n",
        "        # Regex patterns for PII detection\n",
        "        self.patterns = {\n",
        "            'EMAIL': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "            'PHONE': r'(\\+?1[-.\\s]?)?\\(?([0-9]{3})\\)?[-.\\s]?([0-9]{3})[-.\\s]?([0-9]{4})',\n",
        "            'SSN': r'\\b\\d{3}-?\\d{2}-?\\d{4}\\b',\n",
        "            'CREDIT_CARD': r'\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b'\n",
        "        }\n",
        "\n",
        "        # spaCy entity labels we care about\n",
        "        self.spacy_labels = {\n",
        "            'PERSON': 'NAME',\n",
        "            'ORG': 'ORGANIZATION',\n",
        "            'GPE': 'LOCATION',\n",
        "            'LOC': 'LOCATION',\n",
        "            'DATE': 'DATE',\n",
        "            'MONEY': 'MONEY'\n",
        "        }\n",
        "\n",
        "    def _get_next_token(self, label: str) -> str:\n",
        "        \"\"\"Generate next token for a given label type\"\"\"\n",
        "        self.token_counters[label] += 1\n",
        "        return f\"{{{{{label}_{self.token_counters[label]}}}}}\"\n",
        "\n",
        "    def _detect_regex_pii(self, text: str) -> List[PIIMatch]:\n",
        "        \"\"\"Detect PII using regex patterns\"\"\"\n",
        "        matches = []\n",
        "\n",
        "        for label, pattern in self.patterns.items():\n",
        "            for match in re.finditer(pattern, text):\n",
        "                pii_text = match.group()\n",
        "\n",
        "                # Check if we've seen this value before\n",
        "                if pii_text in self.real_to_token:\n",
        "                    token = self.real_to_token[pii_text]\n",
        "                else:\n",
        "                    token = self._get_next_token(label)\n",
        "                    self.real_to_token[pii_text] = token\n",
        "                    self.token_to_real[token] = pii_text\n",
        "\n",
        "                matches.append(PIIMatch(\n",
        "                    text=pii_text,\n",
        "                    label=label,\n",
        "                    start=match.start(),\n",
        "                    end=match.end(),\n",
        "                    token=token\n",
        "                ))\n",
        "\n",
        "        return matches\n",
        "\n",
        "    def _detect_spacy_pii(self, text: str) -> List[PIIMatch]:\n",
        "        \"\"\"Detect PII using spaCy NER\"\"\"\n",
        "        doc = nlp(text)\n",
        "        matches = []\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            if ent.label_ in self.spacy_labels:\n",
        "                pii_text = ent.text\n",
        "                label = self.spacy_labels[ent.label_]\n",
        "\n",
        "                # Check if we've seen this value before\n",
        "                if pii_text in self.real_to_token:\n",
        "                    token = self.real_to_token[pii_text]\n",
        "                else:\n",
        "                    token = self._get_next_token(label)\n",
        "                    self.real_to_token[pii_text] = token\n",
        "                    self.token_to_real[token] = pii_text\n",
        "\n",
        "                matches.append(PIIMatch(\n",
        "                    text=pii_text,\n",
        "                    label=label,\n",
        "                    start=ent.start_char,\n",
        "                    end=ent.end_char,\n",
        "                    token=token\n",
        "                ))\n",
        "\n",
        "        return matches\n",
        "\n",
        "    def anonymize(self, text: str) -> Tuple[str, Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Main anonymization function\n",
        "\n",
        "        Args:\n",
        "            text: Input text to anonymize\n",
        "\n",
        "        Returns:\n",
        "            tuple: (anonymized_text, token_mapping)\n",
        "        \"\"\"\n",
        "        # Detect all PII\n",
        "        regex_matches = self._detect_regex_pii(text)\n",
        "        spacy_matches = self._detect_spacy_pii(text)\n",
        "\n",
        "        # Combine and sort by position (reverse order for replacement)\n",
        "        all_matches = sorted(\n",
        "            regex_matches + spacy_matches,\n",
        "            key=lambda x: x.start,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        # Remove overlapping matches (keep first detected)\n",
        "        filtered_matches = []\n",
        "        for match in all_matches:\n",
        "            overlap = False\n",
        "            for existing in filtered_matches:\n",
        "                if (match.start < existing.end and match.end > existing.start):\n",
        "                    overlap = True\n",
        "                    break\n",
        "            if not overlap:\n",
        "                filtered_matches.append(match)\n",
        "\n",
        "        # Apply replacements\n",
        "        anonymized_text = text\n",
        "        for match in filtered_matches:\n",
        "            anonymized_text = (\n",
        "                anonymized_text[:match.start] +\n",
        "                match.token +\n",
        "                anonymized_text[match.end:]\n",
        "            )\n",
        "\n",
        "        return anonymized_text, dict(self.token_to_real)\n",
        "\n",
        "    def deanonymize(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Restore original values from anonymized text\n",
        "\n",
        "        Args:\n",
        "            text: Anonymized text with tokens\n",
        "\n",
        "        Returns:\n",
        "            str: Text with original values restored\n",
        "        \"\"\"\n",
        "        result = text\n",
        "        for token, real_value in self.token_to_real.items():\n",
        "            result = result.replace(token, real_value)\n",
        "        return result\n",
        "\n",
        "    def get_mapping_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get summary of current mappings for debugging\"\"\"\n",
        "        return {\n",
        "            'token_counts': dict(self.token_counters),\n",
        "            'total_mappings': len(self.token_to_real),\n",
        "            'mappings': dict(self.token_to_real)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "VX0v9ulcW-Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TESTING FRAMEWORK\n",
        "# ============================================================================\n",
        "\n",
        "def test_anonymizer():\n",
        "    \"\"\"Test the anonymizer with sample inputs\"\"\"\n",
        "\n",
        "    anonymizer = PIIAnonymizer()\n",
        "\n",
        "    # Test cases\n",
        "    test_cases = [\n",
        "        \"Hi, I'm John Smith from Acme Corp. Email me at john.smith@acme.com or call 555-123-4567\",\n",
        "        \"Sarah Johnson lives in New York and works at Google. Her SSN is 123-45-6789\",\n",
        "        \"Contact Dr. Michael Brown at michael@hospital.org about the patient in Los Angeles\"\n",
        "    ]\n",
        "\n",
        "    print(\"üîí LLM Privacy Shield - Day 1 Testing\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for i, test_text in enumerate(test_cases, 1):\n",
        "        print(f\"\\nüìù TEST CASE {i}:\")\n",
        "        print(f\"Original: {test_text}\")\n",
        "\n",
        "        # Anonymize\n",
        "        anonymized, mapping = anonymizer.anonymize(test_text)\n",
        "        print(f\"Anonymized: {anonymized}\")\n",
        "\n",
        "        # Test deanonymization\n",
        "        restored = anonymizer.deanonymize(anonymized)\n",
        "        print(f\"Restored: {restored}\")\n",
        "\n",
        "        # Verify restoration\n",
        "        matches_original = restored == test_text\n",
        "        print(f\"‚úÖ Restoration successful: {matches_original}\")\n",
        "\n",
        "        print(f\"\\nToken Mapping:\")\n",
        "        for token, real in mapping.items():\n",
        "            print(f\"  {token} ‚Üê {real}\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Print final summary\n",
        "    print(f\"\\nüìä SUMMARY:\")\n",
        "    summary = anonymizer.get_mapping_summary()\n",
        "    print(f\"Total tokens created: {summary['total_mappings']}\")\n",
        "    print(f\"Token types: {summary['token_counts']}\")\n",
        "\n",
        "    return anonymizer\n"
      ],
      "metadata": {
        "id": "qWTwBUKKXLLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DEMO FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def demo_privacy_shield():\n",
        "    \"\"\"Interactive demo of the privacy shield\"\"\"\n",
        "    print(\"üõ°Ô∏è  LLM Privacy Shield Demo\")\n",
        "    print(\"Enter text to anonymize (or 'quit' to exit):\")\n",
        "\n",
        "    anonymizer = PIIAnonymizer()\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\n> \")\n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "\n",
        "        if user_input.strip():\n",
        "            anonymized, mapping = anonymizer.anonymize(user_input)\n",
        "\n",
        "            print(f\"\\nüîí Anonymized: {anonymized}\")\n",
        "            print(f\"üìã Tokens created: {len(mapping)}\")\n",
        "\n",
        "            if mapping:\n",
        "                print(\"üóùÔ∏è  Token mapping:\")\n",
        "                for token, real in mapping.items():\n",
        "                    print(f\"   {token} ‚Üê {real}\")\n",
        "\n",
        "            # Show what would be sent to LLM\n",
        "            print(f\"\\nüì§ What LLM sees: '{anonymized}'\")\n",
        "\n",
        "            # Show restoration\n",
        "            restored = anonymizer.deanonymize(anonymized)\n",
        "            print(f\"üîì After restoration: '{restored}'\")"
      ],
      "metadata": {
        "id": "8ES1KjFMXRnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# RUN TESTS\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run automated tests\n",
        "    test_anonymizer = test_anonymizer()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Ready for interactive demo!\")\n",
        "    print(\"Call demo_privacy_shield() to try it out\")\n",
        "\n",
        "    # Uncomment to run interactive demo\n",
        "    # demo_privacy_shield()"
      ],
      "metadata": {
        "id": "UgfdZj0GXZeh",
        "outputId": "6eb14d9a-f98a-41ff-e514-fb9bbe5e6e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîí LLM Privacy Shield - Day 1 Testing\n",
            "==================================================\n",
            "\n",
            "üìù TEST CASE 1:\n",
            "Original: Hi, I'm John Smith from Acme Corp. Email me at john.smith@acme.com or call 555-123-4567\n",
            "Anonymized: Hi, I'm {{NAME_1}} from {{ORGANIZATION_1}} Email me at {{EMAIL_1}} or call {{PHONE_1}}\n",
            "Restored: Hi, I'm John Smith from Acme Corp. Email me at john.smith@acme.com or call 555-123-4567\n",
            "‚úÖ Restoration successful: True\n",
            "\n",
            "Token Mapping:\n",
            "  {{EMAIL_1}} ‚Üê john.smith@acme.com\n",
            "  {{PHONE_1}} ‚Üê 555-123-4567\n",
            "  {{NAME_1}} ‚Üê John Smith\n",
            "  {{ORGANIZATION_1}} ‚Üê Acme Corp.\n",
            "------------------------------\n",
            "\n",
            "üìù TEST CASE 2:\n",
            "Original: Sarah Johnson lives in New York and works at Google. Her SSN is 123-45-6789\n",
            "Anonymized: {{NAME_2}} lives in {{LOCATION_1}} and works at {{ORGANIZATION_2}}. Her {{ORGANIZATION_3}} is {{SSN_1}}\n",
            "Restored: Sarah Johnson lives in New York and works at Google. Her SSN is 123-45-6789\n",
            "‚úÖ Restoration successful: True\n",
            "\n",
            "Token Mapping:\n",
            "  {{EMAIL_1}} ‚Üê john.smith@acme.com\n",
            "  {{PHONE_1}} ‚Üê 555-123-4567\n",
            "  {{NAME_1}} ‚Üê John Smith\n",
            "  {{ORGANIZATION_1}} ‚Üê Acme Corp.\n",
            "  {{SSN_1}} ‚Üê 123-45-6789\n",
            "  {{NAME_2}} ‚Üê Sarah Johnson\n",
            "  {{LOCATION_1}} ‚Üê New York\n",
            "  {{ORGANIZATION_2}} ‚Üê Google\n",
            "  {{ORGANIZATION_3}} ‚Üê SSN\n",
            "------------------------------\n",
            "\n",
            "üìù TEST CASE 3:\n",
            "Original: Contact Dr. Michael Brown at michael@hospital.org about the patient in Los Angeles\n",
            "Anonymized: Contact Dr. {{NAME_3}} at {{EMAIL_2}} about the patient in {{LOCATION_2}}\n",
            "Restored: Contact Dr. Michael Brown at michael@hospital.org about the patient in Los Angeles\n",
            "‚úÖ Restoration successful: True\n",
            "\n",
            "Token Mapping:\n",
            "  {{EMAIL_1}} ‚Üê john.smith@acme.com\n",
            "  {{PHONE_1}} ‚Üê 555-123-4567\n",
            "  {{NAME_1}} ‚Üê John Smith\n",
            "  {{ORGANIZATION_1}} ‚Üê Acme Corp.\n",
            "  {{SSN_1}} ‚Üê 123-45-6789\n",
            "  {{NAME_2}} ‚Üê Sarah Johnson\n",
            "  {{LOCATION_1}} ‚Üê New York\n",
            "  {{ORGANIZATION_2}} ‚Üê Google\n",
            "  {{ORGANIZATION_3}} ‚Üê SSN\n",
            "  {{EMAIL_2}} ‚Üê michael@hospital.org\n",
            "  {{NAME_3}} ‚Üê Michael Brown\n",
            "  {{LOCATION_2}} ‚Üê Los Angeles\n",
            "------------------------------\n",
            "\n",
            "üìä SUMMARY:\n",
            "Total tokens created: 12\n",
            "Token types: {'EMAIL': 2, 'PHONE': 1, 'NAME': 3, 'ORGANIZATION': 3, 'SSN': 1, 'LOCATION': 2}\n",
            "\n",
            "==================================================\n",
            "Ready for interactive demo!\n",
            "Call demo_privacy_shield() to try it out\n"
          ]
        }
      ]
    }
  ]
}